# -*- coding: utf-8 -*-
"""hate_Speech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eFCcOfIOYWctAEH2ots-cyVrnGvnd_uI
"""

!pip install evaluate

!pip install --upgrade transformers accelerate peft

!pip uninstall -y transformers
!pip install --upgrade transformers accelerate peft datasets

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import re
import string
from pathlib import Path

from collections import Counter
import random
import operator
from tqdm import tqdm
import time

from wordcloud import WordCloud
from string import punctuation
import nltk
import subprocess

from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, SnowballStemmer

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score

import torch
from torch import nn, optim
from torch.nn import functional as F
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler

from transformers import BertModel, BertTokenizer

from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from transformers import DataCollatorWithPadding
import evaluate

# %matplotlib inline

import kagglehub
import os
import pandas as pd
from google.colab import userdata

# Authenticate with Kaggle using the saved secrets
# Ensure your secrets KAGGLE_USERNAME and KAGGLE_KEY are set up
os.environ["KAGGLE_USERNAME"] = userdata.get('KAGGLE_USERNAME')
os.environ["KAGGLE_KEY"] = userdata.get('KAGGLE_KEY')

# Download the dataset and get the correct Colab path
path = kagglehub.dataset_download("waalbannyantudre/hate-speech-detection-curated-dataset")

# Construct the full, correct file path for Colab
csv_file_path = os.path.join(path, "HateSpeechDatasetBalanced.csv")

print(f"Correct path in Colab is: {csv_file_path}")

# Load the data using the correct path
try:
    data_full = pd.read_csv(csv_file_path)

    print("\n✅ Dataset loaded successfully!")
    display(data_full.head())

except FileNotFoundError:
    print(f"❌ Error: The file was not found at the expected path. Please check the filename.")
except Exception as e:
    print(f"An error occurred: {e}")

data_full.groupby('Label').count()

#The dataset is quite big so we take just 4000 examples of each class
df1 = data_full.query('Label == 0').sample(4000)
df2 = data_full.query('Label == 1').sample(4000)
data = pd.concat([df1, df2], ignore_index=True)

data.shape

combined_title = ' '.join(df1['Content'])

wordcloud_img = WordCloud(width = 800, height = 800,
                            background_color ='white', colormap = 'BuGn',
                            min_font_size = 10).generate(combined_title)

plt.figure(figsize=(10,10))
plt.imshow(wordcloud_img)
plt.axis('off')
plt.title('Frequent words in Non-Hateful Comments')
plt.tight_layout(pad=2)
plt.show()

combined_title = ' '.join(df2['Content'])

wordcloud_img = WordCloud(width = 800, height = 800,
                            background_color ='white', colormap = 'hot_r',
                            min_font_size = 10).generate(combined_title)

plt.figure(figsize=(10,10))
plt.imshow(wordcloud_img)
plt.axis('off')
plt.title('Frequent words in Hateful Comments')
plt.tight_layout(pad=2)
plt.show()

try:
    nltk.data.find('wordnet.zip')
except:
    nltk.download('wordnet', download_dir='/kaggle/working/')
    command = "unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora"
    subprocess.run(command.split())
    nltk.data.path.append('/kaggle/working/')

data_tfidf = data.copy()

import nltk

# Download the list of stopwords
nltk.download('stopwords')

# Download the tokenizer models (needed for word_tokenize)
nltk.download('punkt')

# Download the WordNet database (needed for the lemmatizer)
nltk.download('wordnet')

print("\n✅ NLTK data downloaded successfully!")

stopwords_l = stopwords.words('english')

punctuation = re.compile("[" + re.escape(string.punctuation) + "]")

lemmatizer = WordNetLemmatizer()
stemmer = SnowballStemmer('english') #Snowball stemmer initialised

def text_cleaning(text, mode="stemming"):
    res = []
    text_clean = re.sub(punctuation,'',text)
    tokens = word_tokenize(text_clean)

    for token in tokens:
        if token.lower() not in stopwords_l:
            if mode == "stemming":
                prepared_word = stemmer.stem(token)
            else:
                prepared_word = lemmatizer.lemmatize(token)
            res.append(prepared_word)
    return ' '.join(res)

import nltk
import pandas as pd # Make sure pandas is imported

# Step 1: Download all necessary NLTK data packets
# This ensures they are always available before the cleaning function runs.
# We are also adding 'punkt_tab' as your error message specifically requested it.
nltk.download('punkt')
nltk.download('punkt_tab') # Added based on the error message
nltk.download('stopwords')
nltk.download('wordnet')

print("\n✅ NLTK data is ready.")

# Step 2: Apply your text_cleaning function
# This code will now run without a LookupError.
try:
    print("Applying text cleaning...")
    # Assuming 'data_tfidf' is your DataFrame and 'text_cleaning' is your function
    data_tfidf['cleaned_text'] = data_tfidf['Content'].apply(text_cleaning)
    print("Text cleaning complete.")
    display(data_tfidf.head())
except NameError:
    print("\n❌ Make sure your DataFrame 'data_tfidf' and your 'text_cleaning' function have been defined in a cell above this one.")
except Exception as e:
    print(f"\n❌ An error occurred: {e}")

train, test = train_test_split(data_tfidf, test_size=0.3, stratify=data['Label'], random_state=42)

X_train = train['cleaned_text']
y_train = train['Label']

X_test = test['cleaned_text']
y_test = test['Label']

# TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english')
X_train_vect = tfidf_vectorizer.fit_transform(X_train)

X_test_vect = tfidf_vectorizer.transform(X_test)

linear_clf = LogisticRegression()

# train the model with training data processed using TF-IDF
linear_clf.fit(X_train_vect, y_train)

y_pred_tf_idf = linear_clf.predict(X_test_vect)


report = classification_report(y_test, y_pred_tf_idf)
print(report)

display(pd.DataFrame({"Predicted: Unhateful": confusion_matrix(y_test, y_pred_tf_idf)[:, 0],
              "Predicted: Hateful": confusion_matrix(y_test, y_pred_tf_idf)[:, 1]},
             index=['Actual: Unhateful', 'Actual: Hateful']))

"""Bert"""

train, validation = train_test_split(data, test_size=0.3, stratify=data['Label'], random_state=42)

# Define Dataset
class HateSpeechDataset(Dataset):

    def __init__(self, data):

        # Initialize BERT tokenizer
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        self.data = data


    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        example = self.data.iloc[idx]

        text = example["Content"]
        label = example["Label"]

        # Tokenize the text
        encoding = self.tokenizer.encode_plus(text, padding='max_length', truncation=True, max_length=64, return_tensors='pt')
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"], ##.unsqueeze(0).int(),
            "label": label,
        }

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

dataset_train = HateSpeechDataset(train)
dataset_val = HateSpeechDataset(validation)

#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Create DataLoader
batch_size = 128
dataloader_train = DataLoader(
    dataset_train,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2,
    collate_fn=lambda x: {
        "input_ids": torch.stack([item["input_ids"] for item in x]),
        "attention_mask": torch.stack([item["attention_mask"] for item in x]),
        "labels": torch.tensor([item["label"] for item in x])
    },
    #collate_fn=data_collator,
    pin_memory=True,
)

dataloader_val = DataLoader(
    dataset_val,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2,
    collate_fn=lambda x: {
        "input_ids": torch.stack([item["input_ids"] for item in x]),
        "attention_mask": torch.stack([item["attention_mask"] for item in x]),
        "labels": torch.tensor([item["label"] for item in x])
    },
    #collate_fn=data_collator,
    pin_memory=True,
)

# Define BERT classifier
class BERTClassifier(nn.Module):

    def __init__(self):

        # Specify network layers
        super(BERTClassifier, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')

        self.avg_pool = nn.AdaptiveAvgPool1d(1)

        self.linear = nn.Linear(self.bert.config.hidden_size, 1)

        # Define dropout
        self.dropout = nn.Dropout(0.1)

        # Freeze BERT layers
        for n, p in self.bert.named_parameters():
            p.requires_grad = False

    def forward(self, text, masks):
        #output_bert = self.bert(text, attention_mask=masks).last_hidden_state.mean(axis=1)
        #print(output_bert.last_hidden_state)
        #print(self.bert.config.hidden_size)

        output_bert = self.bert(text, attention_mask=masks).last_hidden_state
        output_bert = self.avg_pool(output_bert.transpose(1, 2)).squeeze(-1)

        return self.linear(self.dropout(output_bert))

model = BERTClassifier()

# Define optimiser, objective function and epochs
optimizer = optim.Adam(model.parameters(), lr=0.001) #optim.AdamW(model.parameters(), lr=5e-5) #
criterion = nn.BCEWithLogitsLoss()
epochs = 5

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model.to(device)

val_losses = []
train_losses = []

# Train model
for epoch_i in range(0, epochs):

    # ========================================
    #               Training
    # ========================================

    model.train()
    print(f"Start training epoch {epoch_i}...")
    total_train_loss = 0
    for i, batch in enumerate(tqdm(dataloader_train)):

        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        masks = batch['attention_mask'].to(device)
        label = batch['labels'].to(device)

        output = model(input_ids, masks)
        loss = criterion(output.squeeze(), label.float())

        loss.backward()

        # Clip the norm of the gradients to 1.0 to prevent the "exploding gradients".
        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(dataloader_train)
    train_losses.append(avg_train_loss)
    # ========================================
    #               Validation
    # ========================================

    model.eval()
    print("Start validation...")
    y_true_bert = list()
    y_pred_bert = list()

    total_eval_loss = 0.0
    with torch.no_grad():
        for batch in dataloader_val:
            input_ids = batch['input_ids'].to(device)
            masks = batch['attention_mask'].to(device)
            label = batch['labels'].to(device)

            output = model(input_ids, masks)
            max_output = (torch.sigmoid(output).cpu().numpy().reshape(-1)>= 0.5).astype(int)
            y_true_bert.extend(label.tolist())
            y_pred_bert.extend(max_output.tolist())

            loss_v = criterion(output.squeeze(), label.float())
            total_eval_loss += loss.item()
    avg_val_loss = total_eval_loss / len(dataloader_val)
    val_losses.append(avg_val_loss)

    print(f"Metrics after Epoch {epoch_i}")
    print(f"Accuracy : {accuracy_score(y_true_bert, y_pred_bert)}")
    print(f"Presision: {np.round(precision_score(y_true_bert, y_pred_bert),3)}")
    print(f"Recall: {np.round(recall_score(y_true_bert, y_pred_bert),3)}")
    print(f"F1: {np.round(f1_score(y_true_bert, y_pred_bert),3)}")
    print("   ")

print('Test accuracy: {:.2f}'.format(accuracy_score(y_true_bert, y_pred_bert)))
print('\nClassification report: \n', classification_report(y_true_bert, y_pred_bert))
print('\nConfusion matrix: \n')
display(pd.DataFrame({"Predicted: Unhateful": confusion_matrix(y_true_bert, y_pred_bert)[:, 0],
              "Predicted: Hateful": confusion_matrix(y_true_bert, y_pred_bert)[:, 1]},
             index=['Actual: Unhateful', 'Actual: Hateful']))

plt.figure(figsize=(10,5))
plt.title("Training and Validation Loss")
plt.plot(val_losses,label="val")
plt.plot(train_losses,label="train")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""Fine-tuning a model with the Trainer API"""

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)
training_args = TrainingArguments(output_dir="test_trainer", report_to="none")

metric_acc = evaluate.load("accuracy")
metric_prec = evaluate.load("precision")
metric_recall = evaluate.load("recall")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    acc = metric_acc.compute(predictions=predictions, references=labels)["accuracy"]
    prec = metric_prec.compute(predictions=predictions, references=labels)["precision"]
    rec = metric_recall.compute(predictions=predictions, references=labels)["recall"]

    return {"accuracy": acc,"precision": prec, "recall": rec}

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset_train,
    eval_dataset=dataset_val,
    compute_metrics=compute_metrics,
    data_collator = data_collator

)

trainer.train()

model.eval()
print("Start validation...")
y_true_auto_bert  = list()
y_pred_auto_bert = list()

total_eval_loss = 0.0
with torch.no_grad():
    for batch in dataloader_val:
        input_ids = batch['input_ids'].to(device)
        masks = batch['attention_mask'].to(device)
        label = batch['labels'].to(device)

        output = model(input_ids, masks)
        max_output = np.argmax(output.logits.cpu().numpy(), axis=-1)
        y_true_auto_bert.extend(label.tolist())
        y_pred_auto_bert.extend(max_output.tolist())


print(f"Accuracy : {accuracy_score(y_true_auto_bert, y_pred_auto_bert)}")
print(f"Presision: {np.round(precision_score(y_true_auto_bert, y_pred_auto_bert),3)}")
print(f"Recall: {np.round(recall_score(y_true_auto_bert, y_pred_auto_bert),3)}")
print(f"F1: {np.round(f1_score(y_true_auto_bert, y_pred_auto_bert),3)}")
print("   ")


print('Test accuracy: {:.2f}'.format(accuracy_score(y_true_auto_bert, y_pred_auto_bert)))
print('\nClassification report: \n', classification_report(y_true_auto_bert, y_pred_auto_bert))
print('\nConfusion matrix: \n')
display(pd.DataFrame({"Predicted: Unhateful": confusion_matrix(y_true_auto_bert, y_pred_auto_bert)[:, 0],
              "Predicted: Hateful": confusion_matrix(y_true_auto_bert, y_pred_auto_bert)[:, 1]},
             index=['Actual: Unhateful', 'Actual: Hateful']))

"""rOBERTA"""

import kagglehub
import os
import pandas as pd
from google.colab import userdata

# Authenticate with Kaggle using the saved secrets
os.environ["KAGGLE_USERNAME"] = userdata.get('KAGGLE_USERNAME')
os.environ["KAGGLE_KEY"] = userdata.get('KAGGLE_KEY')

# Download the dataset and get the correct Colab path
path = kagglehub.dataset_download("waalbannyantudre/hate-speech-detection-curated-dataset")
csv_file_path = os.path.join(path, "HateSpeechDatasetBalanced.csv")

print(f"Correct path in Colab is: {csv_file_path}")

# Load the data using the correct path
try:
    data_full = pd.read_csv(csv_file_path)
    print("\n✅ Raw dataset loaded successfully!")

    # --- FIX: Create the 'final_df' DataFrame ---
    # This is the part that was missing.
    df = data_full.copy() # Work on a copy
    df['HS'] = df['Label'].apply(lambda x: 1 if x == 'hate_speech' else 0)
    df.rename(columns={'Content': 'text'}, inplace=True)
    final_df = df[['text', 'HS']] # Create the final_df

    print("\n✅ 'final_df' created successfully and is ready for visualization!")
    display(final_df.head())

except FileNotFoundError:
    print(f"❌ Error: The file was not found at the expected path.")
except Exception as e:
    print(f"An error occurred: {e}")

# ============== VISUALIZATIONS ==============
# This cell analyzes and plots the distribution of comment lengths and languages.
# ----------------------------------------------------

# Step 1: Install necessary libraries for language detection
!pip install -q langid langcodes

# Step 2: Import all required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import langid
import langcodes
from scipy.stats import gaussian_kde

# Initialize tqdm for pandas so we can see progress bars on .apply()
tqdm.pandas()

# --- Function to analyze and plot word length distribution ---
def word_length_distribution(df: pd.DataFrame):
    """Calculates and plots the distribution of word lengths in a DataFrame."""
    print("Calculating comment length over the given dataset.")

    # Helper function to count words
    def sentence_word_length(sentence: str):
        # Ensure the input is a string to prevent errors
        if not isinstance(sentence, str):
            return 0
        return len(sentence.split())

    # Create a new column with the word count for each comment
    df["WordLength"] = df["text"].progress_apply(sentence_word_length)
    print("Done with sentence length calculation.")

    lengths = df["WordLength"].tolist()

    print("Creating the comment length distribution plot:")
    kde = gaussian_kde(lengths)
    fig, ax = plt.subplots(figsize=(12, 6))

    ax.set_xlabel("Comment length (number of words)")
    ax.set_ylabel("Frequency Density")
    ax.set_title("Comment Length Distribution")

    # Plot the smooth distribution line and the histogram bars
    x = np.linspace(min(lengths), max(lengths), 1000)
    bin_values = np.arange(0, max(lengths) + 10, 10)
    ax.plot(x, kde(x), label="Estimated Distribution")
    ax.hist(lengths, alpha=0.6, edgecolor="black", density=True, bins=bin_values)
    ax.legend()
    ax.set_xticks(bin_values)
    plt.grid(True, axis='y', linestyle='--', alpha=0.7)
    plt.show()

# --- Functions to analyze and plot language distribution ---
def identify_language(sentence: str):
    """Identifies the language of a given sentence."""
    # Ensure the input is a string
    if not isinstance(sentence, str):
        return "Unknown"
    lang, _ = langid.classify(sentence)
    return langcodes.Language.get(lang).display_name()

def language_distribution(df: pd.DataFrame, threshold: float = 0.01):
    """Classifies and plots the language distribution in a DataFrame."""
    print("\nPerforming language classification over the given dataset:")
    df["Language"] = df["text"].progress_apply(identify_language)
    print("Done with language classification.")

    value_counts = df["Language"].value_counts().to_dict()

    # Group small language counts into an 'Other' category
    total = sum(value_counts.values())
    final_counts = {}
    other_count = 0
    # Sort for consistent ordering
    sorted_counts = sorted(value_counts.items(), key=lambda item: item[1], reverse=True)

    for key, value in sorted_counts:
        if value / total >= threshold:
            final_counts[key] = value
        else:
            other_count += value
    if other_count > 0:
        final_counts["Other"] = other_count

    print("Creating the language distribution plot:")
    fig, ax = plt.subplots(figsize=(10, 6))
    languages = list(final_counts.keys())
    counts = list(final_counts.values())

    bars = ax.barh(languages, counts, edgecolor="black")
    ax.set_xlabel("Number of comments")
    ax.set_ylabel("Language")
    ax.set_title("Language Distribution")
    ax.invert_yaxis()  # Display the most common language at the top

    # Add percentage labels to the bars
    for bar in bars:
        width = bar.get_width()
        ax.text(width * 1.01, bar.get_y() + bar.get_height()/2,
                f'{width} ({width/total:.1%})',
                va='center', ha='left')

    ax.set_xlim(right=ax.get_xlim()[1] * 1.2) # Make space for labels
    plt.grid(True, axis='x', linestyle='--', alpha=0.7)
    plt.show()

# --- Execute the analysis on your DataFrame ---
# This code checks if 'final_df' exists before running
if 'final_df' in locals():
    # Use .copy() to avoid SettingWithCopyWarning from pandas
    df_for_eda = final_df.copy()
    word_length_distribution(df_for_eda)
    language_distribution(df_for_eda)
else:
    print("❌ Error: The DataFrame 'final_df' was not found. Please make sure you have run the data loading cells.")

"""LSTM"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import  Input,Dense,Embedding, LSTM,Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from nltk.corpus import stopwords
import re
import matplotlib.pyplot as plt

gpus = tf.config.experimental.list_physical_devices('GPU')

if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

data_full.info()

data_full['Label'].unique()

"""preprocessing data"""

stp = set(stopwords.words("english"))

# Function to preprocess text
def preprocess_text(text):
    text = re.sub("[^a-zA-Z]", " ", text)
    text = text.lower()
    text = text.split()
    words = [word for word in text if word not in stp]
    return " ".join(words)
data_full['ProcessedContent'] = data_full['Content'].apply(preprocess_text)

"""Tokenizing data"""

from keras._tf_keras.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(data_full['ProcessedContent'])
sequences = tokenizer.texts_to_sequences(data_full['ProcessedContent'])

max_len=max([max(0,len(seq)) for seq in sequences])
max_len

"""padding and creating y label"""

from keras._tf_keras.keras.preprocessing.sequence import pad_sequences
x = pad_sequences(sequences, maxlen=max_len)
y = data_full['Label'].values

print(x.shape)
print(y.shape)

"""training and splitting data"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

import random
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

"""creating and applying the model"""

#Creating the model
i = Input(shape=(x_train.shape[1],))
x = Embedding(input_dim=len(tokenizer.word_index)+1,output_dim=20)(i)
x = LSTM(10,return_sequences=True)(x)
x = LSTM(10,return_sequences=True)(x)
x = Flatten()(x)
x = Dense(1,activation='sigmoid')(x)
model = Model(i,x)

# Using Adam optimizer
opt = tf.keras.optimizers.Adam(learning_rate=0.01)

# Compiling the model
model.compile(loss="binary_crossentropy", optimizer=opt, metrics=["accuracy"])

# Fitting the model
history = model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test),batch_size=64)

"""visualization of the model"""

plt.plot(history.history["accuracy"],label="training accuracy")
plt.plot(history.history["val_accuracy"],label="testing accuracy")
plt.title("model accuracy")
plt.xlabel("epoch")
plt.ylabel("accuracy")
plt.legend()
plt.show()

plt.plot(history.history["loss"],label="training loss")
plt.plot(history.history["val_loss"],label="testing loss")
plt.title("model loss")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.legend()
plt.show()

"""predict the model"""

y_pred = model.predict(x_test)
y_pred.shape

y_pred = y_pred.squeeze()

y_pred_binary = np.where(y_pred>0.5,1,0)
y_pred_binary

loss,accuracy = model.evaluate(x_test, y_test)
print("model loss",loss)
print("model accuracy",accuracy)

from sklearn.metrics import accuracy_score,confusion_matrix
print(f"the accuracy score is:{accuracy_score(y_pred_binary,y_test)}")

"""Visualize the confusion matrix"""

import seaborn as sns
con= confusion_matrix(y_pred_binary,y_test)
sns.heatmap(con,annot=True,fmt='d')